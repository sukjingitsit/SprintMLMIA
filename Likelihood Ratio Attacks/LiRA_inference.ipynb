{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "device = torch.device('cpu' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.ids = []\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int]:\n",
    "        id_ = self.ids[index]\n",
    "        img = self.imgs[index]\n",
    "        if not self.transform is None:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[index]\n",
    "        return id_, img, label\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MembershipDataset(TaskDataset):\n",
    "    def __init__(self, transform=None):\n",
    "        super().__init__(transform)\n",
    "        self.membership = []\n",
    "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int, int]:\n",
    "        id_, img, label = super().__getitem__(index)\n",
    "        return id_, img, label, self.membership[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_00 = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.2980, 0.2962, 0.2987], std=[0.2886, 0.2875, 0.2889]),  # Normalize with mean and std\n",
    "])\n",
    "transform_01 = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.2980, 0.2962, 0.2987], std=[0.2886, 0.2875, 0.2889]),  # Normalize with mean and std\n",
    "    transforms.RandomHorizontalFlip(p=1),  # Apply horizontal flip\n",
    "])\n",
    "transform_10 = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.2980, 0.2962, 0.2987], std=[0.2886, 0.2875, 0.2889]),  # Normalize with mean and std\n",
    "    transforms.RandomVerticalFlip(p=1),    # Apply vertical flip\n",
    "])\n",
    "transform_11 = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.2980, 0.2962, 0.2987], std=[0.2886, 0.2875, 0.2889]),  # Normalize with mean and std\n",
    "    transforms.RandomHorizontalFlip(p=1),  # Apply horizontal flip\n",
    "    transforms.RandomVerticalFlip(p=1),    # Apply vertical flip\n",
    "])\n",
    "transform_r = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.2980, 0.2962, 0.2987], std=[0.2886, 0.2875, 0.2889]),  # Normalize with mean and std\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Apply horizontal flip\n",
    "    transforms.RandomVerticalFlip(p=0.5),    # Apply vertical flip\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_27870/4182340037.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  priv_dataset = torch.load('out/data/priv.pt')\n",
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_27870/4182340037.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pub_dataset = torch.load('out/data/pub.pt')\n"
     ]
    }
   ],
   "source": [
    "priv_dataset = torch.load('out/data/priv.pt')\n",
    "pub_dataset = torch.load('out/data/pub.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "priv_loader = DataLoader(priv_dataset, batch_size=16, shuffle=True)\n",
    "pub_loader = DataLoader(pub_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_27870/3429408933.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset_A = torch.load(f\"pretrain_kaggle/split_{i}_A.pt\")\n",
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_27870/3429408933.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset_B = torch.load(f\"pretrain_kaggle/split_{i}_B.pt\")\n",
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_27870/3429408933.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset_A = torch.load(f\"pretrain_kaggle/split_{i}_A.pt\")\n",
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_27870/3429408933.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset_B = torch.load(f\"pretrain_kaggle/split_{i}_B.pt\")\n"
     ]
    }
   ],
   "source": [
    "priv_membership = dict()\n",
    "for i in range(1,33):\n",
    "    dataset_A = torch.load(f\"pretrain_kaggle/split_{i}_A.pt\")\n",
    "    dataset_B = torch.load(f\"pretrain_kaggle/split_{i}_B.pt\")\n",
    "    membership_i = []\n",
    "    for id_ in priv_dataset.ids:\n",
    "        if (i == 1):\n",
    "            priv_membership[id_] = dict()\n",
    "        if id_ in dataset_A.ids:\n",
    "            priv_membership[id_][i] = 'A'\n",
    "        elif id_ in dataset_B.ids:\n",
    "            priv_membership[id_][i] = 'B'\n",
    "pub_membership = dict()\n",
    "for i in range(1,33):\n",
    "    dataset_A = torch.load(f\"pretrain_kaggle/split_{i}_A.pt\")\n",
    "    dataset_B = torch.load(f\"pretrain_kaggle/split_{i}_B.pt\")\n",
    "    membership_i = []\n",
    "    for id_ in pub_dataset.ids:\n",
    "        if (i == 1):\n",
    "            pub_membership[id_] = dict()\n",
    "        if id_ in dataset_A.ids:\n",
    "            pub_membership[id_][i] = 'A'\n",
    "        elif id_ in dataset_B.ids:\n",
    "            pub_membership[id_][i] = 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_epoch = dict()\n",
    "model_epoch[\"1_A\"] = 22\n",
    "model_epoch[\"1_B\"] = 26\n",
    "model_epoch[\"2_A\"] = 25\n",
    "model_epoch[\"2_B\"] = 26\n",
    "model_epoch[\"3_A\"] = 22\n",
    "model_epoch[\"3_B\"] = 26\n",
    "model_epoch[\"4_A\"] = 21\n",
    "model_epoch[\"4_B\"] = 25\n",
    "model_epoch[\"5_A\"] = 27\n",
    "model_epoch[\"5_B\"] = 25\n",
    "model_epoch[\"6_A\"] = 26\n",
    "model_epoch[\"6_B\"] = 25\n",
    "model_epoch[\"7_A\"] = 26\n",
    "model_epoch[\"7_B\"] = 28\n",
    "model_epoch[\"8_A\"] = 27\n",
    "model_epoch[\"8_B\"] = 21\n",
    "model_epoch[\"9_A\"] = 26\n",
    "model_epoch[\"9_B\"] = 24\n",
    "model_epoch[\"10_A\"] = 27\n",
    "model_epoch[\"10_B\"] = 25\n",
    "model_epoch[\"11_A\"] = 24\n",
    "model_epoch[\"11_B\"] = 25\n",
    "model_epoch[\"12_A\"] = 25\n",
    "model_epoch[\"12_B\"] = 23\n",
    "model_epoch[\"13_A\"] = 25\n",
    "model_epoch[\"13_B\"] = 23\n",
    "model_epoch[\"14_A\"] = 27\n",
    "model_epoch[\"14_B\"] = 22\n",
    "model_epoch[\"15_A\"] = 17\n",
    "model_epoch[\"15_B\"] = 22\n",
    "model_epoch[\"16_A\"] = 28\n",
    "model_epoch[\"16_B\"] = 25\n",
    "model_epoch[\"17_A\"] = 26\n",
    "model_epoch[\"17_B\"] = 24\n",
    "model_epoch[\"18_A\"] = 26\n",
    "model_epoch[\"18_B\"] = 27\n",
    "model_epoch[\"19_A\"] = 25\n",
    "model_epoch[\"19_B\"] = 20\n",
    "model_epoch[\"20_A\"] = 23\n",
    "model_epoch[\"20_B\"] = 26\n",
    "model_epoch[\"21_A\"] = 26\n",
    "model_epoch[\"21_B\"] = 25\n",
    "model_epoch[\"22_A\"] = 22\n",
    "model_epoch[\"22_B\"] = 29\n",
    "model_epoch[\"23_A\"] = 25\n",
    "model_epoch[\"23_B\"] = 25\n",
    "model_epoch[\"24_A\"] = 25\n",
    "model_epoch[\"24_B\"] = 25\n",
    "model_epoch[\"25_A\"] = 25\n",
    "model_epoch[\"25_B\"] = 23\n",
    "model_epoch[\"26_A\"] = 22\n",
    "model_epoch[\"26_B\"] = 29\n",
    "model_epoch[\"27_A\"] = 26\n",
    "model_epoch[\"27_B\"] = 25\n",
    "model_epoch[\"28_A\"] = 22\n",
    "model_epoch[\"28_B\"] = 20\n",
    "model_epoch[\"29_A\"] = 24\n",
    "model_epoch[\"29_B\"] = 26\n",
    "model_epoch[\"30_A\"] = 25\n",
    "model_epoch[\"30_B\"] = 24\n",
    "model_epoch[\"31_A\"] = 24\n",
    "model_epoch[\"31_B\"] = 26\n",
    "model_epoch[\"32_A\"] = 25\n",
    "model_epoch[\"32_B\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_27870/2902098367.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  A_data = torch.load(f\"splits/split_{i}_A_output.pt\", map_location=device)\n",
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_27870/2902098367.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  B_data = torch.load(f\"splits/split_{i}_B_output.pt\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model_map = dict()\n",
    "for i in range(1,33):\n",
    "    model_A = resnet18()\n",
    "    model_A.fc = torch.nn.Linear(512, 44)\n",
    "    A_data = torch.load(f\"splits/split_{i}_A_output.pt\", map_location=device)\n",
    "    ckpt_A = A_data[model_epoch[f\"{i}_A\"]][\"state_dict\"]\n",
    "    model_A.load_state_dict(ckpt_A)\n",
    "    model_A.eval()\n",
    "    model_A.to(device)\n",
    "    model_map[f\"{i}_A\"] = model_A\n",
    "    model_B = resnet18()\n",
    "    model_B.fc = torch.nn.Linear(512, 44)\n",
    "    B_data = torch.load(f\"splits/split_{i}_B_output.pt\", map_location=device)\n",
    "    ckpt_B = B_data[model_epoch[f\"{i}_B\"]][\"state_dict\"]\n",
    "    model_B.load_state_dict(ckpt_B)\n",
    "    model_B.eval()\n",
    "    model_B.to(device)\n",
    "    model_map[f\"{i}_B\"] = model_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 images for model 1_A\n",
      "Processed 10000 images for model 1_A\n",
      "Processed 15000 images for model 1_A\n",
      "Processed 20000 images for model 1_A\n",
      "Processed model 1_A\n",
      "Processed 5000 images for model 1_B\n",
      "Processed 10000 images for model 1_B\n",
      "Processed 15000 images for model 1_B\n",
      "Processed 20000 images for model 1_B\n",
      "Processed model 1_B\n",
      "Processed 5000 images for model 2_A\n",
      "Processed 10000 images for model 2_A\n",
      "Processed 15000 images for model 2_A\n",
      "Processed 20000 images for model 2_A\n",
      "Processed model 2_A\n",
      "Processed 5000 images for model 2_B\n",
      "Processed 10000 images for model 2_B\n",
      "Processed 15000 images for model 2_B\n",
      "Processed 20000 images for model 2_B\n",
      "Processed model 2_B\n",
      "Processed 5000 images for model 3_A\n",
      "Processed 10000 images for model 3_A\n",
      "Processed 15000 images for model 3_A\n",
      "Processed 20000 images for model 3_A\n",
      "Processed model 3_A\n",
      "Processed 5000 images for model 3_B\n",
      "Processed 10000 images for model 3_B\n",
      "Processed 15000 images for model 3_B\n",
      "Processed 20000 images for model 3_B\n",
      "Processed model 3_B\n",
      "Processed 5000 images for model 4_A\n",
      "Processed 10000 images for model 4_A\n",
      "Processed 15000 images for model 4_A\n",
      "Processed 20000 images for model 4_A\n",
      "Processed model 4_A\n",
      "Processed 5000 images for model 4_B\n",
      "Processed 10000 images for model 4_B\n",
      "Processed 15000 images for model 4_B\n",
      "Processed 20000 images for model 4_B\n",
      "Processed model 4_B\n",
      "Processed 5000 images for model 5_A\n",
      "Processed 10000 images for model 5_A\n",
      "Processed 15000 images for model 5_A\n",
      "Processed 20000 images for model 5_A\n",
      "Processed model 5_A\n",
      "Processed 5000 images for model 5_B\n",
      "Processed 10000 images for model 5_B\n",
      "Processed 15000 images for model 5_B\n",
      "Processed 20000 images for model 5_B\n",
      "Processed model 5_B\n",
      "Processed 5000 images for model 6_A\n",
      "Processed 10000 images for model 6_A\n",
      "Processed 15000 images for model 6_A\n",
      "Processed 20000 images for model 6_A\n",
      "Processed model 6_A\n",
      "Processed 5000 images for model 6_B\n",
      "Processed 10000 images for model 6_B\n",
      "Processed 15000 images for model 6_B\n",
      "Processed 20000 images for model 6_B\n",
      "Processed model 6_B\n",
      "Processed 5000 images for model 7_A\n",
      "Processed 10000 images for model 7_A\n",
      "Processed 15000 images for model 7_A\n",
      "Processed 20000 images for model 7_A\n",
      "Processed model 7_A\n",
      "Processed 5000 images for model 7_B\n",
      "Processed 10000 images for model 7_B\n",
      "Processed 15000 images for model 7_B\n",
      "Processed 20000 images for model 7_B\n",
      "Processed model 7_B\n",
      "Processed 5000 images for model 8_A\n",
      "Processed 10000 images for model 8_A\n",
      "Processed 15000 images for model 8_A\n",
      "Processed 20000 images for model 8_A\n",
      "Processed model 8_A\n",
      "Processed 5000 images for model 8_B\n",
      "Processed 10000 images for model 8_B\n",
      "Processed 15000 images for model 8_B\n",
      "Processed 20000 images for model 8_B\n",
      "Processed model 8_B\n",
      "Processed 5000 images for model 9_A\n",
      "Processed 10000 images for model 9_A\n",
      "Processed 15000 images for model 9_A\n",
      "Processed 20000 images for model 9_A\n",
      "Processed model 9_A\n",
      "Processed 5000 images for model 9_B\n",
      "Processed 10000 images for model 9_B\n",
      "Processed 15000 images for model 9_B\n",
      "Processed 20000 images for model 9_B\n",
      "Processed model 9_B\n",
      "Processed 5000 images for model 10_A\n",
      "Processed 10000 images for model 10_A\n",
      "Processed 15000 images for model 10_A\n",
      "Processed 20000 images for model 10_A\n",
      "Processed model 10_A\n",
      "Processed 5000 images for model 10_B\n",
      "Processed 10000 images for model 10_B\n",
      "Processed 15000 images for model 10_B\n",
      "Processed 20000 images for model 10_B\n",
      "Processed model 10_B\n",
      "Processed 5000 images for model 11_A\n",
      "Processed 10000 images for model 11_A\n",
      "Processed 15000 images for model 11_A\n",
      "Processed 20000 images for model 11_A\n",
      "Processed model 11_A\n",
      "Processed 5000 images for model 11_B\n",
      "Processed 10000 images for model 11_B\n",
      "Processed 15000 images for model 11_B\n",
      "Processed 20000 images for model 11_B\n",
      "Processed model 11_B\n",
      "Processed 5000 images for model 12_A\n",
      "Processed 10000 images for model 12_A\n",
      "Processed 15000 images for model 12_A\n",
      "Processed 20000 images for model 12_A\n",
      "Processed model 12_A\n",
      "Processed 5000 images for model 12_B\n",
      "Processed 10000 images for model 12_B\n",
      "Processed 15000 images for model 12_B\n",
      "Processed 20000 images for model 12_B\n",
      "Processed model 12_B\n",
      "Processed 5000 images for model 13_A\n",
      "Processed 10000 images for model 13_A\n",
      "Processed 15000 images for model 13_A\n",
      "Processed 20000 images for model 13_A\n",
      "Processed model 13_A\n",
      "Processed 5000 images for model 13_B\n",
      "Processed 10000 images for model 13_B\n",
      "Processed 15000 images for model 13_B\n",
      "Processed 20000 images for model 13_B\n",
      "Processed model 13_B\n",
      "Processed 5000 images for model 14_A\n",
      "Processed 10000 images for model 14_A\n",
      "Processed 15000 images for model 14_A\n",
      "Processed 20000 images for model 14_A\n",
      "Processed model 14_A\n",
      "Processed 5000 images for model 14_B\n",
      "Processed 10000 images for model 14_B\n",
      "Processed 15000 images for model 14_B\n",
      "Processed 20000 images for model 14_B\n",
      "Processed model 14_B\n",
      "Processed 5000 images for model 15_A\n",
      "Processed 10000 images for model 15_A\n",
      "Processed 15000 images for model 15_A\n",
      "Processed 20000 images for model 15_A\n",
      "Processed model 15_A\n",
      "Processed 5000 images for model 15_B\n",
      "Processed 10000 images for model 15_B\n",
      "Processed 15000 images for model 15_B\n",
      "Processed 20000 images for model 15_B\n",
      "Processed model 15_B\n",
      "Processed 5000 images for model 16_A\n",
      "Processed 10000 images for model 16_A\n",
      "Processed 15000 images for model 16_A\n",
      "Processed 20000 images for model 16_A\n",
      "Processed model 16_A\n",
      "Processed 5000 images for model 16_B\n",
      "Processed 10000 images for model 16_B\n",
      "Processed 15000 images for model 16_B\n",
      "Processed 20000 images for model 16_B\n",
      "Processed model 16_B\n",
      "Processed 5000 images for model 17_A\n",
      "Processed 10000 images for model 17_A\n",
      "Processed 15000 images for model 17_A\n",
      "Processed 20000 images for model 17_A\n",
      "Processed model 17_A\n",
      "Processed 5000 images for model 17_B\n",
      "Processed 10000 images for model 17_B\n",
      "Processed 15000 images for model 17_B\n",
      "Processed 20000 images for model 17_B\n",
      "Processed model 17_B\n",
      "Processed 5000 images for model 18_A\n",
      "Processed 10000 images for model 18_A\n",
      "Processed 15000 images for model 18_A\n",
      "Processed 20000 images for model 18_A\n",
      "Processed model 18_A\n",
      "Processed 5000 images for model 18_B\n",
      "Processed 10000 images for model 18_B\n",
      "Processed 15000 images for model 18_B\n",
      "Processed 20000 images for model 18_B\n",
      "Processed model 18_B\n",
      "Processed 5000 images for model 19_A\n",
      "Processed 10000 images for model 19_A\n",
      "Processed 15000 images for model 19_A\n",
      "Processed 20000 images for model 19_A\n",
      "Processed model 19_A\n",
      "Processed 5000 images for model 19_B\n",
      "Processed 10000 images for model 19_B\n",
      "Processed 15000 images for model 19_B\n",
      "Processed 20000 images for model 19_B\n",
      "Processed model 19_B\n",
      "Processed 5000 images for model 20_A\n",
      "Processed 10000 images for model 20_A\n",
      "Processed 15000 images for model 20_A\n",
      "Processed 20000 images for model 20_A\n",
      "Processed model 20_A\n",
      "Processed 5000 images for model 20_B\n",
      "Processed 10000 images for model 20_B\n",
      "Processed 15000 images for model 20_B\n",
      "Processed 20000 images for model 20_B\n",
      "Processed model 20_B\n",
      "Processed 5000 images for model 21_A\n",
      "Processed 10000 images for model 21_A\n",
      "Processed 15000 images for model 21_A\n",
      "Processed 20000 images for model 21_A\n",
      "Processed model 21_A\n",
      "Processed 5000 images for model 21_B\n",
      "Processed 10000 images for model 21_B\n",
      "Processed 15000 images for model 21_B\n",
      "Processed 20000 images for model 21_B\n",
      "Processed model 21_B\n",
      "Processed 5000 images for model 22_A\n",
      "Processed 10000 images for model 22_A\n",
      "Processed 15000 images for model 22_A\n",
      "Processed 20000 images for model 22_A\n",
      "Processed model 22_A\n",
      "Processed 5000 images for model 22_B\n",
      "Processed 10000 images for model 22_B\n",
      "Processed 15000 images for model 22_B\n",
      "Processed 20000 images for model 22_B\n",
      "Processed model 22_B\n",
      "Processed 5000 images for model 23_A\n",
      "Processed 10000 images for model 23_A\n",
      "Processed 15000 images for model 23_A\n",
      "Processed 20000 images for model 23_A\n",
      "Processed model 23_A\n",
      "Processed 5000 images for model 23_B\n",
      "Processed 10000 images for model 23_B\n",
      "Processed 15000 images for model 23_B\n",
      "Processed 20000 images for model 23_B\n",
      "Processed model 23_B\n",
      "Processed 5000 images for model 24_A\n",
      "Processed 10000 images for model 24_A\n",
      "Processed 15000 images for model 24_A\n",
      "Processed 20000 images for model 24_A\n",
      "Processed model 24_A\n",
      "Processed 5000 images for model 24_B\n",
      "Processed 10000 images for model 24_B\n",
      "Processed 15000 images for model 24_B\n",
      "Processed 20000 images for model 24_B\n",
      "Processed model 24_B\n",
      "Processed 5000 images for model 25_A\n",
      "Processed 10000 images for model 25_A\n",
      "Processed 15000 images for model 25_A\n",
      "Processed 20000 images for model 25_A\n",
      "Processed model 25_A\n",
      "Processed 5000 images for model 25_B\n",
      "Processed 10000 images for model 25_B\n",
      "Processed 15000 images for model 25_B\n",
      "Processed 20000 images for model 25_B\n",
      "Processed model 25_B\n",
      "Processed 5000 images for model 26_A\n",
      "Processed 10000 images for model 26_A\n",
      "Processed 15000 images for model 26_A\n",
      "Processed 20000 images for model 26_A\n",
      "Processed model 26_A\n",
      "Processed 5000 images for model 26_B\n",
      "Processed 10000 images for model 26_B\n",
      "Processed 15000 images for model 26_B\n",
      "Processed 20000 images for model 26_B\n",
      "Processed model 26_B\n",
      "Processed 5000 images for model 27_A\n",
      "Processed 10000 images for model 27_A\n",
      "Processed 15000 images for model 27_A\n",
      "Processed 20000 images for model 27_A\n",
      "Processed model 27_A\n",
      "Processed 5000 images for model 27_B\n",
      "Processed 10000 images for model 27_B\n",
      "Processed 15000 images for model 27_B\n",
      "Processed 20000 images for model 27_B\n",
      "Processed model 27_B\n",
      "Processed 5000 images for model 28_A\n",
      "Processed 10000 images for model 28_A\n",
      "Processed 15000 images for model 28_A\n",
      "Processed 20000 images for model 28_A\n",
      "Processed model 28_A\n",
      "Processed 5000 images for model 28_B\n",
      "Processed 10000 images for model 28_B\n",
      "Processed 15000 images for model 28_B\n",
      "Processed 20000 images for model 28_B\n",
      "Processed model 28_B\n",
      "Processed 5000 images for model 29_A\n",
      "Processed 10000 images for model 29_A\n",
      "Processed 15000 images for model 29_A\n",
      "Processed 20000 images for model 29_A\n",
      "Processed model 29_A\n",
      "Processed 5000 images for model 29_B\n",
      "Processed 10000 images for model 29_B\n",
      "Processed 15000 images for model 29_B\n",
      "Processed 20000 images for model 29_B\n",
      "Processed model 29_B\n",
      "Processed 5000 images for model 30_A\n",
      "Processed 10000 images for model 30_A\n",
      "Processed 15000 images for model 30_A\n",
      "Processed 20000 images for model 30_A\n",
      "Processed model 30_A\n",
      "Processed 5000 images for model 30_B\n",
      "Processed 10000 images for model 30_B\n",
      "Processed 15000 images for model 30_B\n",
      "Processed 20000 images for model 30_B\n",
      "Processed model 30_B\n",
      "Processed 5000 images for model 31_A\n",
      "Processed 10000 images for model 31_A\n",
      "Processed 15000 images for model 31_A\n",
      "Processed 20000 images for model 31_A\n",
      "Processed model 31_A\n",
      "Processed 5000 images for model 31_B\n",
      "Processed 10000 images for model 31_B\n",
      "Processed 15000 images for model 31_B\n",
      "Processed 20000 images for model 31_B\n",
      "Processed model 31_B\n",
      "Processed 5000 images for model 32_A\n",
      "Processed 10000 images for model 32_A\n",
      "Processed 15000 images for model 32_A\n",
      "Processed 20000 images for model 32_A\n",
      "Processed model 32_A\n",
      "Processed 5000 images for model 32_B\n",
      "Processed 10000 images for model 32_B\n",
      "Processed 15000 images for model 32_B\n",
      "Processed 20000 images for model 32_B\n",
      "Processed model 32_B\n"
     ]
    }
   ],
   "source": [
    "priv_in_scores = dict()\n",
    "priv_out_scores = dict()\n",
    "for key in model_epoch.keys():\n",
    "    model = model_map[key]\n",
    "    processed = 0\n",
    "    for id_, img, label in zip(priv_dataset.ids, priv_dataset.imgs, priv_dataset.labels):\n",
    "        img_00 = transform_00(img).unsqueeze(0)\n",
    "        img_01 = transform_01(img).unsqueeze(0)\n",
    "        img_10 = transform_10(img).unsqueeze(0)\n",
    "        img_11 = transform_11(img).unsqueeze(0)\n",
    "        imgs_ = torch.cat((img_00, img_01, img_10, img_11), dim=0)\n",
    "        outputs = model(imgs_)\n",
    "        raw_scores = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        #print(raw_scores.shape)\n",
    "        confidence_score = raw_scores[:, label]\n",
    "        #print(confidence_score.shape)\n",
    "        logit_score = torch.log(confidence_score/(1-confidence_score))\n",
    "        #print(logit_score.shape)\n",
    "        #print(logit_score)\n",
    "        if priv_membership[id_][int(key[:-2])]==key[-1]:\n",
    "            if id_ not in priv_in_scores:\n",
    "                priv_in_scores[id_] = list()\n",
    "            priv_in_scores[id_].extend(logit_score.detach().cpu().numpy())\n",
    "        else:\n",
    "            if id_ not in priv_out_scores:\n",
    "                priv_out_scores[id_] = list()\n",
    "            priv_out_scores[id_].extend(logit_score.detach().cpu().numpy())\n",
    "        processed += 1\n",
    "        if processed % 5000 == 0:\n",
    "            print(f\"Processed {processed} images for model {key}\")\n",
    "    print(f\"Processed model {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(priv_in_scores, \"splits/in_scores.pt\")\n",
    "torch.save(priv_out_scores, \"splits/out_scores.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathwaysasuke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
