{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from torch.utils.data import Dataset, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.ids = []\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int]:\n",
    "        id_ = self.ids[index]\n",
    "        img = self.imgs[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[index]\n",
    "        return id_, img, label\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MembershipDataset(TaskDataset):\n",
    "    def __init__(self, transform=None):\n",
    "        super().__init__(transform)\n",
    "        self.membership = []\n",
    "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int, int]:\n",
    "        id_, img, label = super().__getitem__(index)\n",
    "        return id_, img, label, self.membership[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_balanced_splits(dataset, num_classes, split_size, alternate_odd=True):\n",
    "    class_indices = {i: [] for i in range(num_classes)}\n",
    "    for idx in range(len(dataset)):\n",
    "        _, _, label, _ = dataset[idx]\n",
    "        class_indices[label].append(idx)\n",
    "    indices_A, indices_B = [], []\n",
    "    odd_flag = True\n",
    "    for label, indices in class_indices.items():\n",
    "        random.shuffle(indices)\n",
    "        mid = len(indices) // 2\n",
    "        if len(indices) % 2 == 1 and alternate_odd:\n",
    "            if odd_flag:\n",
    "                indices_A.extend(indices[:mid + 1])\n",
    "                indices_B.extend(indices[mid + 1:])\n",
    "            else:\n",
    "                indices_A.extend(indices[:mid])\n",
    "                indices_B.extend(indices[mid:])\n",
    "            odd_flag = not odd_flag\n",
    "        else:\n",
    "            indices_A.extend(indices[:mid])\n",
    "            indices_B.extend(indices[mid:])\n",
    "    random.shuffle(indices_A)\n",
    "    random.shuffle(indices_B)\n",
    "    return Subset(dataset, indices_A[:split_size]), Subset(dataset, indices_B[:split_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataframe(dataset, dataframe, splits, num):\n",
    "    dataframe[f\"split_{num}\"] = \"\"\n",
    "    membership = np.array([\"\" for _ in range(len(dataset))], dtype=object)\n",
    "    membership[splits[0]] = \"A\"\n",
    "    membership[splits[1]] = \"B\"\n",
    "    dataframe[f\"split_{num}\"] = membership\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save(dataset, num_classes=44, num_splits=8, output_path=\".\", name=\"priv\"):\n",
    "    data = {\n",
    "        \"id\": [dataset.ids[i] for i in range(len(dataset))],\n",
    "        \"label\": [dataset.labels[i] for i in range(len(dataset))]\n",
    "    }\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    for i in range(1, num_splits + 1):\n",
    "        print(f\"Split {i} for {name} started\")\n",
    "        dataset_A, dataset_B = create_class_balanced_splits(dataset, num_classes, len(dataset) // 2)\n",
    "        torch.save(dataset_A, f\"{output_path}/split_{i}_A_{name}.pt\")\n",
    "        torch.save(dataset_B, f\"{output_path}/split_{i}_B_{name}.pt\")\n",
    "        dataframe = update_dataframe(dataset, dataframe, (dataset_A.indices, dataset_B.indices), i)\n",
    "    dataframe.to_csv(f\"{output_path}/membership_splits_{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_36483/203693147.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pub_dataset = torch.load(\"out/data/pub.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 for pub started\n",
      "Split 2 for pub started\n",
      "Split 3 for pub started\n",
      "Split 4 for pub started\n",
      "Split 5 for pub started\n",
      "Split 6 for pub started\n",
      "Split 7 for pub started\n",
      "Split 8 for pub started\n",
      "Split 9 for pub started\n",
      "Split 10 for pub started\n",
      "Split 11 for pub started\n",
      "Split 12 for pub started\n",
      "Split 13 for pub started\n",
      "Split 14 for pub started\n",
      "Split 15 for pub started\n",
      "Split 16 for pub started\n",
      "Split 17 for pub started\n",
      "Split 18 for pub started\n",
      "Split 19 for pub started\n",
      "Split 20 for pub started\n",
      "Split 21 for pub started\n",
      "Split 22 for pub started\n",
      "Split 23 for pub started\n",
      "Split 24 for pub started\n",
      "Split 25 for pub started\n",
      "Split 26 for pub started\n",
      "Split 27 for pub started\n",
      "Split 28 for pub started\n",
      "Split 29 for pub started\n",
      "Split 30 for pub started\n",
      "Split 31 for pub started\n",
      "Split 32 for pub started\n"
     ]
    }
   ],
   "source": [
    "pub_dataset = torch.load(\"out/data/pub.pt\")\n",
    "process_and_save(pub_dataset, num_classes=44, num_splits=32, output_path=\"./outputs\", name=\"pub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_36483/4120355365.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  priv_dataset = torch.load(\"out/data/priv.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 for priv started\n",
      "Split 2 for priv started\n",
      "Split 3 for priv started\n",
      "Split 4 for priv started\n",
      "Split 5 for priv started\n",
      "Split 6 for priv started\n",
      "Split 7 for priv started\n",
      "Split 8 for priv started\n",
      "Split 9 for priv started\n",
      "Split 10 for priv started\n",
      "Split 11 for priv started\n",
      "Split 12 for priv started\n",
      "Split 13 for priv started\n",
      "Split 14 for priv started\n",
      "Split 15 for priv started\n",
      "Split 16 for priv started\n",
      "Split 17 for priv started\n",
      "Split 18 for priv started\n",
      "Split 19 for priv started\n",
      "Split 20 for priv started\n",
      "Split 21 for priv started\n",
      "Split 22 for priv started\n",
      "Split 23 for priv started\n",
      "Split 24 for priv started\n",
      "Split 25 for priv started\n",
      "Split 26 for priv started\n",
      "Split 27 for priv started\n",
      "Split 28 for priv started\n",
      "Split 29 for priv started\n",
      "Split 30 for priv started\n",
      "Split 31 for priv started\n",
      "Split 32 for priv started\n"
     ]
    }
   ],
   "source": [
    "priv_dataset = torch.load(\"out/data/priv.pt\")\n",
    "process_and_save(priv_dataset, num_classes=44, num_splits=32, output_path=\"./outputs\", name=\"priv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creation(pub_dataset, priv_dataset, num):\n",
    "    priv_A = torch.load(f'outputs/split_{num}_A_priv.pt')\n",
    "    priv_B = torch.load(f'outputs/split_{num}_B_priv.pt')\n",
    "    pub_A = torch.load(f'outputs/split_{num}_A_pub.pt')\n",
    "    pub_B = torch.load(f'outputs/split_{num}_B_pub.pt')\n",
    "    dataset_A = MembershipDataset()\n",
    "    dataset_B = MembershipDataset()\n",
    "    for index in priv_A.indices:\n",
    "        dataset_A.ids.append(priv_dataset.ids[index])\n",
    "        dataset_A.imgs.append(priv_dataset.imgs[index])\n",
    "        dataset_A.labels.append(priv_dataset.labels[index])\n",
    "        dataset_A.membership.append(priv_dataset.membership[index])\n",
    "    for index in pub_A.indices:\n",
    "        dataset_A.ids.append(pub_dataset.ids[index])\n",
    "        dataset_A.imgs.append(pub_dataset.imgs[index])\n",
    "        dataset_A.labels.append(pub_dataset.labels[index])\n",
    "        dataset_A.membership.append(pub_dataset.membership[index])\n",
    "    for index in priv_B.indices:\n",
    "        dataset_B.ids.append(priv_dataset.ids[index])\n",
    "        dataset_B.imgs.append(priv_dataset.imgs[index])\n",
    "        dataset_B.labels.append(priv_dataset.labels[index])\n",
    "        dataset_B.membership.append(priv_dataset.membership[index])\n",
    "    for index in pub_B.indices:\n",
    "        dataset_B.ids.append(pub_dataset.ids[index])\n",
    "        dataset_B.imgs.append(pub_dataset.imgs[index])\n",
    "        dataset_B.labels.append(pub_dataset.labels[index])\n",
    "        dataset_B.membership.append(pub_dataset.membership[index])\n",
    "    torch.save(dataset_A, f'pretrain/split_{num}_A.pt')\n",
    "    torch.save(dataset_B, f'pretrain/split_{num}_B.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_datasets(pub_dataset, priv_dataset, num_splits):\n",
    "    for i in range(1, num_splits + 1):\n",
    "        dataset_creation(pub_dataset, priv_dataset, i)\n",
    "        print(f\"Split {i} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_36483/172781969.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  priv_A = torch.load(f'outputs/split_{num}_A_priv.pt')\n",
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_36483/172781969.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  priv_B = torch.load(f'outputs/split_{num}_B_priv.pt')\n",
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_36483/172781969.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pub_A = torch.load(f'outputs/split_{num}_A_pub.pt')\n",
      "/var/folders/rq/9d8zfn5n4_503rc3q_gkyglw0000gr/T/ipykernel_36483/172781969.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pub_B = torch.load(f'outputs/split_{num}_B_pub.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 completed\n",
      "Split 2 completed\n",
      "Split 3 completed\n",
      "Split 4 completed\n",
      "Split 5 completed\n",
      "Split 6 completed\n",
      "Split 7 completed\n",
      "Split 8 completed\n",
      "Split 9 completed\n",
      "Split 10 completed\n",
      "Split 11 completed\n",
      "Split 12 completed\n",
      "Split 13 completed\n",
      "Split 14 completed\n",
      "Split 15 completed\n",
      "Split 16 completed\n",
      "Split 17 completed\n",
      "Split 18 completed\n",
      "Split 19 completed\n",
      "Split 20 completed\n",
      "Split 21 completed\n",
      "Split 22 completed\n",
      "Split 23 completed\n",
      "Split 24 completed\n",
      "Split 25 completed\n",
      "Split 26 completed\n",
      "Split 27 completed\n",
      "Split 28 completed\n",
      "Split 29 completed\n",
      "Split 30 completed\n",
      "Split 31 completed\n",
      "Split 32 completed\n"
     ]
    }
   ],
   "source": [
    "all_datasets(pub_dataset, priv_dataset, num_splits=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathwaysasuke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
