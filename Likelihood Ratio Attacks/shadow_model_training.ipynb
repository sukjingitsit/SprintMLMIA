{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10588790,"sourceType":"datasetVersion","datasetId":6522583}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport copy\nfrom typing import Tuple\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import resnet18\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n!rm -rf /kaggle/working/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TaskDataset(Dataset):\n    def __init__(self, transform=None):\n        self.ids = []\n        self.imgs = []\n        self.labels = []\n        self.transform = transform\n    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int]:\n        id_ = self.ids[index]\n        img = self.imgs[index]\n        if not self.transform is None:\n            img = self.transform(img)\n        label = self.labels[index]\n        return id_, img, label\n    def __len__(self):\n        return len(self.ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MembershipDataset(TaskDataset):\n    def __init__(self, transform=None):\n        super().__init__(transform)\n        self.membership = []\n    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int, int]:\n        id_, img, label = super().__getitem__(index)\n        if self.membership[index] is None:\n            return id_, img, label, -1\n        return id_, img, label, self.membership[index]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform_00 = transforms.Compose([\n    transforms.Normalize(mean=[0.2980, 0.2962, 0.2987], std=[0.2886, 0.2875, 0.2889]),  # Normalize with mean and std\n])\ntransform_01 = transforms.Compose([\n    transforms.Normalize(mean=[0.2980, 0.2962, 0.2987], std=[0.2886, 0.2875, 0.2889]),  # Normalize with mean and std\n    transforms.RandomHorizontalFlip(p=1),  # Apply horizontal flip\n])\ntransform_10 = transforms.Compose([\n    transforms.Normalize(mean=[0.2980, 0.2962, 0.2987], std=[0.2886, 0.2875, 0.2889]),  # Normalize with mean and std\n    transforms.RandomVerticalFlip(p=1),    # Apply vertical flip\n])\ntransform_11 = transforms.Compose([\n    transforms.Normalize(mean=[0.2980, 0.2962, 0.2987], std=[0.2886, 0.2875, 0.2889]),  # Normalize with mean and std\n    transforms.RandomHorizontalFlip(p=1),  # Apply horizontal flip\n    transforms.RandomVerticalFlip(p=1),    # Apply vertical flip\n])\ntransform_r = transforms.Compose([\n    transforms.Normalize(mean=[0.2980, 0.2962, 0.2987], std=[0.2886, 0.2875, 0.2889]),  # Normalize with mean and std\n    transforms.RandomHorizontalFlip(p=0.5),  # Apply horizontal flip\n    transforms.RandomVerticalFlip(p=0.5),    # Apply vertical flip\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#ckpt = torch.load('/kaggle/input/sprintmodel/attack_model.pt', map_location=device)\nmodel_A = resnet18(weights=None)\nmodel_A.fc = torch.nn.Linear(512, 44)\n#model_A.load_state_dict(ckpt)\nmodel_A.to(device)\nmodel_A.train()\nmodel_B = resnet18(weights=None)\nmodel_B.fc = torch.nn.Linear(512, 44)\n#model_B.load_state_dict(ckpt)\nmodel_B.to(device)\nmodel_B.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#priv_dataset = torch.load('out/data/priv.pt')\n#pub_dataset = torch.load('out/data/pub.pt')\ndata_num = 31\ndataset_A = torch.load(f'/kaggle/input/sprintml-dataset-1/pretrain_kaggle/split_{data_num}_A.pt')\ndataset_B = torch.load(f'/kaggle/input/sprintml-dataset-1/pretrain_kaggle/split_{data_num}_B.pt')\ndataloader_A = DataLoader(dataset_A, batch_size=128, shuffle=True)\ndataloader_B = DataLoader(dataset_B, batch_size=128, shuffle=True)\ndataset_AA = torch.load(f'/kaggle/input/sprintml-dataset-1/pretrain_kaggle/split_{data_num}_A.pt')\ndataset_BB = torch.load(f'/kaggle/input/sprintml-dataset-1/pretrain_kaggle/split_{data_num}_B.pt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer_A = torch.optim.Adam(model_A.parameters(), lr=1e-3)\ncriterion_A = torch.nn.CrossEntropyLoss()\noptimizer_B = torch.optim.Adam(model_B.parameters(), lr=1e-3)\ncriterion_B = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, dataloader_1, dataset_1, dataset_2, optimizer, criterion, epochs, batch_size, path):\n    save_dict = dict()\n    for epoch in range(epochs):\n        model.train()\n        train_samples = 0\n        for ids, imgs, labels, memberships in dataloader_1:\n            #fimgs = []\n            #fimgs.extend([transform_00(img) for img in imgs])\n            #fimgs.extend([transform_01(img) for img in imgs])\n            #fimgs.extend([transform_10(img) for img in imgs])\n            #fimgs.extend([transform_11(img) for img in imgs])\n            imgs = [transform_r(img) for img in imgs]\n            train_samples += len(imgs)\n            imgs = torch.stack(imgs).to(device)\n            labels = labels.to(device)\n            preds = model(imgs)\n            loss = criterion(preds, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            #print(f'Epoch {epoch+1}/{epochs}, batch {i//batch_size+1}/{-(-len(dataset_1) // batch_size)}, loss {loss.item()}')\n        model.eval()\n        total_correct_in = 0\n        total_correct_out = 0\n        total_samples_in = 0\n        total_samples_out = 0\n        for i in range(0, len(dataset_1), batch_size):\n            batch = dataset_1[i:i+batch_size]\n            ids, imgs, labels, membership  = batch\n            imgs = torch.stack([transform_00(img) for img in imgs]).to(device)\n            labels = torch.tensor(labels).to(device)\n            preds = model(imgs)\n            correct = torch.where(preds.argmax(dim=1) == labels, 1, 0).sum().item()\n            total_correct_in += correct\n            total_samples_in += len(labels)\n            #print(f'Accuracy: {correct/len(labels)}')\n        for i in range(0, len(dataset_2), batch_size):\n            batch = dataset_2[i:i+batch_size]\n            ids, imgs, labels, membership  = batch\n            imgs = torch.stack([transform_00(img) for img in imgs]).to(device)\n            labels = torch.tensor(labels).to(device)\n            preds = model(imgs)\n            correct = torch.where(preds.argmax(dim=1) == labels, 1, 0).sum().item()\n            total_correct_out += correct\n            total_samples_out += len(labels)\n            #print(f'Accuracy: {correct/len(labels)}')\n        acc_in = total_correct_in / total_samples_in\n        acc_out = total_correct_out / total_samples_out\n        #print(train_samples, total_samples_in, total_samples_out)\n        print(f'Epoch {epoch+1}/{epochs}, accuracy in: {acc_in}, accuracy out: {acc_out}')\n        save_dict[epoch+1] = {'in_accuracy' : acc_in, 'out_accuracy' : acc_out, 'state_dict' : copy.deepcopy(model.state_dict())}\n    torch.save(save_dict, path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model(model_A, dataloader_A, dataset_AA, dataset_BB, optimizer_A, criterion_A, 40, 256, f'/kaggle/working/split_{data_num}_A_output.pt')\ntrain_model(model_B, dataloader_B, dataset_BB, dataset_AA, optimizer_B, criterion_B, 40, 256, f'/kaggle/working/split_{data_num}_B_output.pt')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}